diff --git a/src/main/java/org/jsoup/helper/DataUtil.java b/src/main/java/org/jsoup/helper/DataUtil.java
index 4bdf4fe..392c2dc 100644
--- a/src/main/java/org/jsoup/helper/DataUtil.java
+++ b/src/main/java/org/jsoup/helper/DataUtil.java
@@ -80,7 +80,7 @@ public class DataUtil {
             Element meta = doc.select("meta[http-equiv=content-type], meta[charset]").first();
             if (meta != null) { // if not found, will keep utf-8 as best attempt
                 String foundCharset = meta.hasAttr("http-equiv") ? getCharsetFromContentType(meta.attr("content")) : meta.attr("charset");
-                if (foundCharset != null && foundCharset.length() != 0 && !foundCharset.equals(defaultCharset)) { // need to re-decode
+                if (foundCharset != null && foundCharset.length() != 0 && !foundCharset.equalsIgnoreCase(defaultCharset)) { // need to re-decode
                     charsetName = foundCharset;
                     byteData.rewind();
                     docData = Charset.forName(foundCharset).decode(byteData).toString();
@@ -95,13 +95,18 @@ public class DataUtil {
             // there are times where there is a spurious byte-order-mark at the start of the text. Shouldn't be present
             // in utf-8. If after decoding, there is a BOM, strip it; otherwise will cause the parser to go straight
             // into head mode
-
+            if (docData.length() > 0 && docData.charAt(0) == 65533) {
+                docData = docData.substring(1);
+            }
             doc = parser.parseInput(docData, baseUri);
             doc.outputSettings().charset(charsetName);
         }
         return doc;
     }
-
+One of the most common problems you’ll find when working on real-world data problems is dirty data. This can come in the form of missing values, inconsistent formatting, nonsensical ranges, outliers, and so on. This often requires a lot of cleaning and wrangling, which can be tedious. Sometimes you have to figure out whether a data point is an anomaly and remove it or fill in a placeholder value for it, or you may have to spot any columns or rows with null values and decide how to fix that.
+Dirty data can impact your results in many different ways. For instance, it can impact your analysis by throwing off your statistics or creating incorrect data visualizations.
+There’s an art to creating code to handle dirty data. You’ll need to figure out how to handle these edge cases and create a workflow that fits into your overall workflow.
+A strategy for handling dirty data is to decide how to handle each type of dirty data you encounter. Do you want to drop any rows with null values, or fill in those null values with a placeholder value like 0? Do you want to flag any values that are outside of a certain range as outliers, or do you just want to drop them?
     static ByteBuffer readToByteBuffer(InputStream inStream) throws IOException {
         byte[] buffer = new byte[bufferSize];
         ByteArrayOutputStream outStream = new ByteArrayOutputStream(bufferSize);
